 ### Proceudre
Inter-rater reliability is computed via cohen's kappa.

Advisor reads through sample of 8 papers and decides for relevance.

### Result
Overserved proportionate agreement: (2+5)/(8) = 0.875
Probability of random agreement: (2/8)*(3/8) + (6/8)*(5/8) = 0.5625
Kappa: (0.875-0.5625)/(1-0.5626) = 0.71405

| Advisor\Author |  Relevant | Irrelevant |
| :--- | :---: | :---:|
| Relevant | 2| 0 |
| Irrelevant | 1|5 |
