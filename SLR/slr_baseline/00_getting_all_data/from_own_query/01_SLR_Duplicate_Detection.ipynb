{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "commercial-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import editdistance\n",
    "from utils import retrieve_paper_details\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acceptable-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We detect duplicates between the papers by checking their title.\n",
    "# In order to also detect variations of the same title (e.g. Uppercase, Added Punctuation)\n",
    "# we employ the levensthein edit distance with a max difference of 20.\n",
    "# Since some of the titles have a length of less than 20 characters, the will naturally be\n",
    "# flagged as duplicates. However, this is not a problem, as we manually inspect the duplicates afterward.\n",
    "def find_duplicates(papers):\n",
    "    duplicates = []\n",
    "    for idx in range(len(papers)):\n",
    "        cur_paper = papers[idx]\n",
    "        relevant_papers = papers[idx+1:]\n",
    "        for paper in relevant_papers:\n",
    "            if editdistance.eval(paper.titel.lower(),cur_paper.titel.lower()) < 20:\n",
    "                duplicates.append((cur_paper, paper))\n",
    "    return duplicates\n",
    "\n",
    "# An alternative approach towards detecting duplicates is by checking for their DOI. \n",
    "# However, since not all paper entries retrieved from the databases contain DOI's,\n",
    "# this measurement is less acurate. It is still a nice baseline to see how well the\n",
    "# other deduplication approach performs.\n",
    "def find_duplicates_via_doi(papers):\n",
    "    duplicates = []\n",
    "    for idx in range(len(papers)):\n",
    "        cur_paper = papers[idx]\n",
    "        relevant_papers = papers[idx+1:]\n",
    "        for paper in relevant_papers:\n",
    "            if cur_paper.doi != \"\" and paper.doi == cur_paper.doi:\n",
    "                duplicates.append((cur_paper, paper))\n",
    "    return duplicates\n",
    "\n",
    "# Write all duplicates do csv file for manual inspection\n",
    "def summarize_duplicates(duplicates):\n",
    "    keys = duplicates[0][0].__dict__.keys()\n",
    "    with open('results/automatically_duplicated_papers.csv', 'w') as f:\n",
    "        csv_file = csv.DictWriter(f, keys, delimiter=\";\")\n",
    "        csv_file.writeheader()\n",
    "        for duplicate in duplicates:\n",
    "            csv_file.writerow(duplicate[0].__dict__)\n",
    "            csv_file.writerow(duplicate[1].__dict__)\n",
    "            csv_file.writerow(defaultdict(str)) # Empty line added to differentiate between duplicate pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "metropolitan-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 885\n"
     ]
    }
   ],
   "source": [
    "resource_dir = \"resources/\"\n",
    "resource_files = ['acm.csv', 'IEEE.csv', 'ScienceDirect.csv', 'scopus.csv']\n",
    "\n",
    "papers = retrieve_paper_details(resource_dir, resource_files)\n",
    "print('Total number of papers:', len(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fancy-circle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates via Title: 116\n",
      "Number of duplicates via DOI: 90\n"
     ]
    }
   ],
   "source": [
    "duplicates = find_duplicates(papers)\n",
    "duplicates_via_doi = find_duplicates_via_doi(papers)\n",
    "print('Number of duplicates via Title:', len(duplicates))\n",
    "print('Number of duplicates via DOI:', len(duplicates_via_doi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "specialized-musician",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Are all duplicates found via DOI also found via Title?\n",
      "Answer: True\n"
     ]
    }
   ],
   "source": [
    "issubset = set(duplicates_via_doi).issubset(set(duplicates))\n",
    "print('Question: Are all duplicates found via DOI also found via Title?\\nAnswer:', issubset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "furnished-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_duplicates(duplicates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
